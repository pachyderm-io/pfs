syntax = "proto3";
package pps;

import "google/protobuf/empty.proto";
import "google/protobuf/timestamp.proto";
import "google/protobuf/duration.proto";

import "gogoproto/gogo.proto";

import "client/pfs/pfs.proto";

option go_package = "pps";

message Secret {
  // Name must be the name of the secret in kubernetes.
  string name = 1;
  string mount_path = 2;
}

message Transform {
  string image = 1;
  repeated string cmd = 2;
  map<string, string> env = 3;
  repeated Secret secrets = 4;
  repeated string image_pull_secrets = 9;
  repeated string stdin = 5;
  repeated int64 accept_return_code = 6;
  bool debug = 7;
}

message Egress {
  string URL = 1;
}

message Job {
  string id = 1 [(gogoproto.customname) = "ID"];
}

enum JobState {
  JOB_STARTING = 0;
  JOB_RUNNING = 1;
  JOB_FAILURE = 2;
  JOB_SUCCESS = 3;
  JOB_KILLED = 4;
}

message Service {
  int32 internal_port = 1;
  int32 external_port = 2;
}

message AtomInput {
  string name = 1;
  string repo = 2;
  string branch = 3;
  string commit = 4;
  string glob = 5;
  bool lazy = 6;
  string from_commit = 7;
}

message Input {
  AtomInput atom = 1;
  repeated Input cross = 2;
  repeated Input union = 3;
}

message JobInput {
  string name = 4;
  pfs.Commit commit = 1;
  string glob = 2;
  bool lazy = 3;
}

message ParallelismSpec {
  reserved 1;

  // Starts the pipeline/job with a 'constant' workers, unless 'constant' is
  // zero. If 'constant' is zero (which is the zero value of ParallelismSpec),
  // then Pachyderm will choose the number of workers that is started,
  // (currently it chooses the number of workers in the cluster)
  uint64 constant = 2;

  // Starts the pipeline/job with number of workers equal to 'coefficient' * N,
  // where N is the number of nodes in the kubernetes cluster.
  //
  // For example, if each Kubernetes node has four CPUs, you might set
  // 'coefficient' to four, so that there are four Pachyderm workers per
  // Kubernetes node, and each Pachyderm worker gets one CPU. If you want to
  // reserve half the nodes in your cluster for other tasks, you might set
  // 'coefficient' to 0.5.
  double coefficient = 3;
}

message InputFile {
  // This file's absolute path within its pfs repo.
  string path = 4;

  // This file's hash
  bytes hash = 5;
}

message Datum {
  // ID is the hash computed from all the files
  string id = 1 [(gogoproto.customname) = "ID"];
  Job job = 2;
}

enum DatumState {
    FAILED = 0;
    SUCCESS = 1;
    SKIPPED = 2;
}

message DatumInfo {
   Datum datum = 1;
   DatumState state = 2;
   ProcessStats stats = 3;
   pfs.File pfs_state = 4;
}

message DatumInfos {
  repeated DatumInfo datum_info = 1;
}


message Aggregate {
  int64 count = 1;
  double mean = 2;
  double stddev = 3;
  double fifth_percentile = 4;
  double ninety_fifth_percentile = 5;
}

message ProcessStats {
  google.protobuf.Duration download_time = 1;
  google.protobuf.Duration process_time = 2;
  google.protobuf.Duration upload_time = 3;
  uint64 download_bytes = 4;
  uint64 upload_bytes = 5;
}

message AggregateProcessStats {
  Aggregate download_time = 1;
  Aggregate process_time = 2;
  Aggregate upload_time = 3;
  Aggregate download_bytes = 4;
  Aggregate upload_bytes = 5;
}

message WorkerStatus {
  string worker_id = 1 [(gogoproto.customname) = "WorkerID"];
  string job_id = 2 [(gogoproto.customname) = "JobID"];
  repeated pps.InputFile data = 3;
  // Started is the time processing on the current datum began.
  google.protobuf.Timestamp started = 4;
  ProcessStats stats = 5;
  int64 queue_size = 6;
}

// ResourceSpec describes the amount of resources that pipeline pods should
// request from kubernetes, for scheduling.
message ResourceSpec {
  // The number of CPUs each worker needs (partial values are allowed, and
  // encouraged)
  float cpu = 1;

  // The amount of memory, in bytes, each worker needs (in bytes, with allowed
  // SI suffixes (M, K, G, Mi, Ki, Gi, etc).
  string memory = 2;

  // The number of GPUs each worker needs.
  int64 gpu = 3;
}

message BatchSpec {
  // the max number of datums that will be run in a batch
  int64 count_max = 1;

  // the maximum number of 
  int64 size_max = 2;
}

message JobInfo {
  reserved 4;
  Job job = 1;
  Transform transform = 2;
  Pipeline pipeline = 3;
  uint64 pipeline_version = 13;
  ParallelismSpec parallelism_spec = 12;
  repeated JobInput inputs = 5;
  Egress egress = 15;
  Job parent_job = 6;
  google.protobuf.Timestamp started = 7;
  google.protobuf.Timestamp finished = 8;
  pfs.Commit output_commit = 9;
  JobState state = 10;
  Service service = 14;
  pfs.Repo output_repo = 18;
  string output_branch = 17;
  uint64 restart = 20;
  int64 data_processed = 22;
  int64 data_skipped = 30;
  int64 data_total = 23;
  ProcessStats stats = 31;
  repeated WorkerStatus worker_status = 24;
  ResourceSpec resource_spec = 25;
  Input input = 26;
  pfs.BranchInfo new_branch = 27;
  bool incremental = 28;
  pfs.Commit stats_commit = 29;
  bool enable_stats = 32;
  string salt = 33;
  BatchSpec batch = 34;
}

enum WorkerState {
  POD_RUNNING = 0;
  POD_SUCCESS = 1;
  POD_FAILED = 2;
}

message Worker {
  string name = 1;
  WorkerState state = 2;
}

message JobInfos {
  repeated JobInfo job_info = 1;
}

message Pipeline {
  string name = 1;
}

message PipelineInput {
  string name = 5;
  pfs.Repo repo = 1;
  string branch = 2;
  string glob = 3;
  bool lazy = 4;
  pfs.Commit from = 6;
}

enum PipelineState {
  // When the pipeline is not ready to be triggered by commits.
  // This happens when either 1) a pipeline has been created but not
  // yet picked up by a PPS server, or 2) the pipeline does not have
  // any inputs and is meant to be triggered manually
  PIPELINE_STARTING = 0;
  // After this pipeline is picked up by a pachd node.  This is the normal
  // state of a pipeline.
  PIPELINE_RUNNING = 1;
  // After some error caused runPipeline to exit, but before the
  // pipeline is re-run.  This is when the exponential backoff is
  // in effect.
  PIPELINE_RESTARTING = 2;
  // We have retried too many times and we have given up on this pipeline.
  PIPELINE_FAILURE = 3;
  // The pipeline has been explicitly paused by the user.
  PIPELINE_PAUSED = 4;
}

message PipelineInfo {
  reserved 3;
  string id = 17 [(gogoproto.customname) = "ID"];
  Pipeline pipeline = 1;
  uint64 version = 11;
  Transform transform = 2;
  ParallelismSpec parallelism_spec = 10;
  repeated PipelineInput inputs = 4;
  Egress egress = 15;
  google.protobuf.Timestamp created_at = 6;
  PipelineState state = 7;
  string recent_error = 8;
  map<int32, int32> job_counts = 9;
  string output_branch = 16;
  google.protobuf.Duration scale_down_threshold = 18;
  ResourceSpec resource_spec = 19;
  Input input = 20;
  string description = 21;
  bool incremental = 22;
  string cache_size = 23;
  bool enable_stats = 24;
  string salt = 25;
  string capability = 26;
  BatchSpec batch = 27;
}

message PipelineInfos {
  repeated PipelineInfo pipeline_info = 1;
}

message CreateJobRequest {
  reserved 3;
  Transform transform = 1;
  Pipeline pipeline = 2;
  uint64 pipeline_version = 10;
  ParallelismSpec parallelism_spec = 7;
  repeated JobInput inputs = 4;
  Egress egress = 9;
  // When service is defined, we create a long running job
  // by using a k8s RC and Service instead of a k8s Job
  Service service = 8;
  pfs.Repo output_repo = 12;
  string output_branch = 11;
  Job parent_job = 13;
  ResourceSpec resource_spec = 14;
  Input input = 15;
  pfs.BranchInfo new_branch = 16;
  bool incremental = 17;
  bool enable_stats = 18;
  string salt = 19;
  BatchSpec batch = 20;
}

message InspectJobRequest {
  Job job = 1;
  bool block_state = 2; // block until state is either JOB_STATE_FAILURE or JOB_STATE_SUCCESS
}

message ListJobRequest {
  Pipeline pipeline = 1; // nil means all pipelines
  repeated pfs.Commit input_commit = 2; // nil means all inputs
}

message DeleteJobRequest {
  Job job = 1;
}

message StopJobRequest {
  Job job = 1;
}

message GetLogsRequest {
  // The pipeline from which we want to get logs (required if the job in 'job'
  // was created as part of a pipeline. To get logs from a non-orphan job
  // without the pipeline that created it, you need to use ElasticSearch).
  Pipeline pipeline = 2;

  // The job from which we want to get logs.
  Job job = 1;

  // Names of input files from which we want processing logs. This may contain
  // multiple files, to query pipelines that contain multiple inputs. Each
  // filter may be an absolute path of a file within a pps repo, or it may be
  // a hash for that file (to search for files at specific versions)
  repeated string data_filters = 3;

  string input_file_id = 4 [(gogoproto.customname) = "InputFileID"];

  // If true get logs from the master process
  bool master = 5;
}

// LogMessage is a log line from a PPS worker, annotated with metadata
// indicating when and why the line was logged.
message LogMessage {
  // The job and pipeline for which a PFS file is being processed (if the job
  // is an orphan job, pipeline name and ID will be unset)
  string pipeline_name = 1;
  string job_id = 3 [(gogoproto.customname) = "JobID"];
  string worker_id = 7 [(gogoproto.customname) = "WorkerID"];
  string input_file_id = 9 [(gogoproto.customname) = "InputFileID"];
  bool master = 10;

  // The PFS files being processed (one per pipeline/job input)
  repeated InputFile data = 4;

  // User is true if log message comes from the users code.
  bool user = 8;

  // The message logged, and the time at which it was logged
  google.protobuf.Timestamp ts = 5;
  string message = 6;
}

message RestartDatumRequest {
  Job job = 1;
  repeated string data_filters = 2;
}

message InspectDatumRequest {
  Datum datum = 1;
}

message ListDatumRequest {
  Job job = 1;
}

message CreatePipelineRequest {
  reserved 3;
  Pipeline pipeline = 1;
  Transform transform = 2;
  ParallelismSpec parallelism_spec = 7;
  repeated PipelineInput inputs = 4;
  Egress egress = 9;
  bool update = 5;
  string output_branch = 10;
  google.protobuf.Duration scale_down_threshold = 11;
  ResourceSpec resource_spec = 12;
  Input input = 13;
  string description = 14;
  bool incremental = 15;
  string cache_size = 16;
  bool enable_stats = 17;
  // Reprocess forces the pipeline to reprocess all datums.
  // It only has meaning if Update is true
  bool reprocess = 18;
  BatchSpec batch = 19;
}

message InspectPipelineRequest {
  Pipeline pipeline = 1;
}

message ListPipelineRequest {
}

message DeletePipelineRequest {
  Pipeline pipeline = 1;
  bool delete_jobs = 2;
  bool delete_repo = 3;
  bool all = 4;
}

message StartPipelineRequest {
  Pipeline pipeline = 1;
}

message StopPipelineRequest {
  Pipeline pipeline = 1;
}

message RerunPipelineRequest {
  Pipeline pipeline = 1;
  repeated pfs.Commit exclude = 2;
  repeated pfs.Commit include = 3;
}

message GarbageCollectRequest {}
message GarbageCollectResponse {}

service API {
  rpc CreateJob(CreateJobRequest) returns (Job) {}
  rpc InspectJob(InspectJobRequest) returns (JobInfo) {}
  rpc ListJob(ListJobRequest) returns (JobInfos) {}
  rpc DeleteJob(DeleteJobRequest) returns (google.protobuf.Empty) {}
  rpc StopJob(StopJobRequest) returns (google.protobuf.Empty) {}
  rpc InspectDatum(InspectDatumRequest) returns (DatumInfo) {}
  rpc ListDatum(ListDatumRequest) returns (DatumInfos) {}
  rpc RestartDatum(RestartDatumRequest) returns (google.protobuf.Empty) {}

  rpc CreatePipeline(CreatePipelineRequest) returns (google.protobuf.Empty) {}
  rpc InspectPipeline(InspectPipelineRequest) returns (PipelineInfo) {}
  rpc ListPipeline(ListPipelineRequest) returns (PipelineInfos) {}
  rpc DeletePipeline(DeletePipelineRequest) returns (google.protobuf.Empty) {}
  rpc StartPipeline(StartPipelineRequest) returns (google.protobuf.Empty) {}
  rpc StopPipeline(StopPipelineRequest) returns (google.protobuf.Empty) {}
  rpc RerunPipeline(RerunPipelineRequest) returns (google.protobuf.Empty) {}

  // DeleteAll deletes everything
  rpc DeleteAll(google.protobuf.Empty) returns (google.protobuf.Empty) {}
  rpc GetLogs(GetLogsRequest) returns (stream LogMessage) {}

  // Garbage collection
  rpc GarbageCollect(GarbageCollectRequest) returns (GarbageCollectResponse) {}
}
